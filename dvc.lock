schema: '2.0'
stages:
  build-tokenizer:
    cmd: python scripts/train_tokenizer.py --vocab-size 30000
    deps:
    - path: data/hf-datasets/wmt14
      md5: 314a5483a3ffa3d3596aa8ca765639ee.dir
      size: 15098090914
      nfiles: 36
    - path: scripts/train_tokenizer.py
      md5: 57eb4f5df01695bc0ccccf13eb76490d
      size: 3193
    outs:
    - path: data/tokenizer
      md5: 46c9ecb5c7cc3fa7699eb3c3f07c027f.dir
      size: 1656261
      nfiles: 2
